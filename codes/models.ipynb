{"cells":[{"cell_type":"markdown","metadata":{"id":"RZxsyH2Amu_x"},"source":["1.服务器挂载数据集及模块引用"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAyjlKEemobf"},"outputs":[],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1654927701095,"user":{"displayName":"张哲","userId":"17432761019385598781"},"user_tz":-480},"id":"DxQfW9lf4Ct5"},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/') #切换目录，这个是网盘的根目录\n","os.getcwd()#获取当前目录\n","os.chdir('商家推荐项目') #切换到data目录"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"awmTs3XyDHQN"},"outputs":[],"source":["pip install gensim==4.1.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wu4SdEuitp5b"},"outputs":[],"source":["import re\n","import time\n","import math\n","import jieba\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","from gensim.models import KeyedVectors, word2vec\n","from torch.utils.data import DataLoader, WeightedRandomSampler\n","from torch.utils.data.dataset import TensorDataset\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, accuracy_score"]},{"cell_type":"markdown","metadata":{"id":"Hzntrk2qm400"},"source":["2.数据预处理"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyQAiX4cqRJQ"},"outputs":[],"source":["def get_attr_name(attr):\n","    # 根据索引获取属性名\n","    a = ['location_traffic_convenience', 'location_distance_from_business_district', 'location_easy_to_find',\n","         'service_wait_time', 'service_waiters_attitude', 'service_parking_convenience', 'service_serving_speed',\n","         'price_level', 'price_cost_effective', 'price_discount',\n","         'environment_decoration', 'environment_noise', 'environment_space', 'environment_cleaness',\n","         'dish_portion', 'dish_taste', 'dish_look', 'dish_recommendation',\n","         'others_overall_experience', 'others_willing_to_consume_again'][attr - 2]\n","    return a\n","\n","def Get_Label(attr, file_path='./data/train/train.csv'):\n","    # 将csv文件转化为[属性分类列表]\n","    label = []\n","    proportion = [0 for i in range(4)]\n","    a = get_attr_name(attr)\n","    raw_data = pd.read_csv(file_path, usecols=[attr])\n","    safer, pointer = 1, 0  # 内存守护者\n","    for index, row in raw_data.iterrows():\n","        # if row[a] == -2: continue\n","        proportion[row[a]+2] += 1\n","        # pointer += 1\n","        # if pointer % safer != 0: continue\n","        label.append(row[a]+2)\n","    max_class = max(proportion)\n","    proportion = [max_class / i for i in proportion]\n","    return label, proportion\n","    \n","def StopwordLoader(file_path='./data/stopwords.txt'):\n","    stopword = set(line.strip() for line in open(file_path, encoding='UTF-8').readlines())\n","    return stopword"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l706qpjLTpph"},"outputs":[],"source":["#读取文本并分析化\n","stop_word = StopwordLoader()\n","def Context_Tokenizer(attr, file_path='./data/train/train.csv', result_path='./trainer.txt'):\n","    # 将csv文件中的'context'转化为[[分词列表],...]\n","    sentences = []\n","    raw_data = pd.read_csv(file_path, usecols=[1, attr])\n","    a = get_attr_name(attr)\n","    for index, row in raw_data.iterrows():\n","        # if row[a] == -2: continue\n","        content = row['content']\n","        c_len, poi = len(content), 0\n","        while poi<c_len:\n","            if content[poi] == '\\n' or content[poi] == ' ' or content[poi] == '\\t' or content[poi] == '\\r':\n","                content = content[:poi] + content[poi+1:]\n","                c_len -= 1\n","            else: poi += 1\n","        word_list = [k for k in jieba.cut(content)]\n","        words = []\n","        for i in range(len(word_list)):\n","            if word_list[i] not in stop_word:\n","                words.append(word_list[i])\n","        sentences.append(words)\n","    with open(result_path,'w',encoding='utf-8') as f:\n","        for sentence in sentences:\n","            f.write(' '.join(sentence)+'\\n')\n","    return sentences\n","\n","def Context_Frequency(sentences, file_path='./frequency.txt'):\n","    # 统计分词结果中的词频，并写入文件\n","    word_dict = {}\n","    for sentence in sentences:\n","        for word in sentence:\n","            if word not in word_dict.keys():\n","                word_dict[word] = 1\n","            else: word_dict[word] += 1\n","    # word_dict = sorted(list(word_dict.items()),key=lambda x: x[1],reverse=True) #按词频进行排序\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        f.writelines(str(word_dict))\n","\n","if __name__=='__main__':\n","    attr = 8\n","    sentence1 = Context_Tokenizer(attr)\n","    sentence2 = Context_Tokenizer(attr, file_path='./data/valid/valid.csv', result_path='./valid.txt')\n","    for item in sentence2:\n","        sentence1.append(item)\n","    Context_Frequency(sentence1)"]},{"cell_type":"markdown","metadata":{"id":"3PzO9zbJnLUM"},"source":["3.模型一(尝试)：基于词典的未提及类别预识别"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UO3OqRkZ5fVs"},"outputs":[],"source":["#词嵌入向量模型\n","def read_and_train(file_path='./trainer.txt', model_path='./trainer.model'):\n","    # 从文本文件中读取分词结果，并提交给word2vec进行训练，并保存word2vec模型\n","    sentences = []\n","    with open(file_path,'r',encoding='utf-8') as f:\n","        while True:\n","            line = f.readline()\n","            if len(line) == 0: break\n","            sentence = line.split()\n","            sentences.append(sentence)\n","    model = word2vec.Word2Vec(sentences, workers=10, min_count=0, vector_size=300, alpha=0.025, window=4, negative=4)\n","    model.save(model_path)\n","\n","def continue_train(file_path='./valid.txt', model_path='./trainer.model'):\n","    sentences = []\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        while True:\n","            line = f.readline()\n","            if len(line) == 0: break\n","            sentence = line.split()\n","            sentences.append(sentence)\n","    model=word2vec.Word2Vec.load(model_path)\n","    model.build_vocab(sentences, update=True)\n","    model.train(sentences, total_examples=model.corpus_count, epochs=model.epochs)\n","    model.save(model_path)\n","\n","if __name__=='__main__':\n","    read_and_train()\n","    continue_train()\n","    models = word2vec.Word2Vec.load('./trainer.model')\n","    print(models.wv['好吃'])\n","    print(models.wv['雌性'])\n","    print(models.wv['crasyones'])\n","    print(models.wv['缸内'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SDmw1TLeh1rY"},"outputs":[],"source":["#统计特定话题的共同高频词表\n","def oov(attr, file_path='./data/train/train.csv'):\n","  raw_data = pd.read_csv(file_path, usecols=[1,attr])\n","  a = get_attr_name(attr)\n","  mention, oov = dict(), set()\n","  for index, row in raw_data.iterrows():\n","        content = row['content']\n","        c_len, poi = len(content), 0\n","        while poi<c_len:\n","            if content[poi] == '\\n' or content[poi] == ' ' or content[poi] == '\\t' or content[poi] == '\\r':\n","                content = content[:poi] + content[poi+1:]\n","                c_len -= 1\n","            else: poi += 1\n","        word_set = set(k for k in jieba.cut(content))\n","        if row[a] == -2: oov = oov.union(word_set)\n","        else:\n","          for word in word_set:\n","            if word not in mention.keys(): mention[word]=1\n","            else: mention[word] += 1\n","  return mention,oov\n","\n","if __name__=='__main__':\n","  attr = 8\n","  save_name = './' + get_attr_name(attr) + '_mention.txt'\n","  dic1,oov1 = oov(attr)\n","  dic2,oov2 = oov(attr, file_path='./data/valid/valid.csv')\n","  dic = {**dic1,**dic2}\n","  for word in oov1:\n","    if word in dic:\n","      del dic[word]\n","  for word in oov2:\n","    if word in dic:\n","      del dic[word]\n","  dic = sorted(list(dic.items()),key=lambda y:y[1],reverse=True)\n","  with open(save_name, 'w', encoding='utf-8') as f:\n","    for word, fre in dic:\n","      if fre < 3: break\n","      f.write(word+' ')"]},{"cell_type":"markdown","metadata":{"id":"QUDPxdAjnXUT"},"source":["4.模型二(尝试)：基于MLP的未提及类别识别"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JwRC5IHKw1GZ"},"outputs":[],"source":["#尝试利用线性分类器的方法将-2与{-1,0,1}识别出来\n","class Mention(nn.Module):\n","  def __init__(self,vocab_len,batch_size=10,embed_dim=300):\n","    super(Mention,self).__init__()\n","    # 词嵌入模型预训练\n","    embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","    word2idx = {'None': 0}\n","    vocab_list = [(k, embedding_model.wv[k]) for k, v in embedding_model.wv.key_to_index.items()]\n","    embeddings_matrix = np.zeros((len(embedding_model.wv.key_to_index.items()) + 1, embedding_model.vector_size))\n","    for i in range(len(vocab_list)):\n","      word = vocab_list[i][0]\n","      word2idx[word] = i + 1\n","      embeddings_matrix[i + 1] = vocab_list[i][1]\n","    embeddings_matrix = torch.tensor(embeddings_matrix, dtype=torch.float)\n","    # 初始化keras中的Embedding层权重\n","    self.embedding_table = nn.Embedding(len(embeddings_matrix), embed_dim, _weight= embeddings_matrix)\n","    self.embedding_table.requires_grad_ = False\n","    #全连接层\n","    self.b=batch_size\n","    self.linear=nn.Linear(60000,300)\n","    self.linear2=nn.Linear(300,2)\n","  def forward(self,x):\n","    x=self.embedding_table(x.long())\n","    x=torch.reshape(x,(self.b,60000))\n","    logit2=F.relu(self.linear(x))\n","    logit=F.relu(self.linear2(logit2))\n","    return logit\n","\n","def mention_2_tensor(attr, context_path='./trainer.txt', file_path='./data/train/train.csv',batch_size=10):\n","  raw_data = pd.read_csv(file_path, usecols=[attr])\n","  a=get_attr_name(attr)\n","  labels=[]\n","  port=[0,0]\n","  for index, row in raw_data.iterrows():\n","    if row[a]==-2: \n","      labels.append(0)\n","      port[0]+=1\n","    else: \n","      labels.append(1)\n","      port[1]+=1\n","  port=[1/item for item in port]\n","  vocab = dict()\n","  vocab['None'] = 0\n","  embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","  vocab_list = [k for k,_ in embedding_model.wv.key_to_index.items()]\n","  for i in range(len(vocab_list)):\n","    word = vocab_list[i]\n","    vocab[word] = i + 1\n","  data = []\n","  max_len=200\n","  with open(context_path, 'r', encoding='utf-8') as f:\n","      pointer = 0\n","      while True:\n","          line = f.readline()\n","          cent = [0 for i in range(max_len)]\n","          if len(line) == 0: break\n","          sentence = line.split()\n","          for index,word in enumerate(sentence):\n","            if index >= max_len: break\n","            if word not in vocab.keys():\n","                vocab[word] = len(vocab)\n","                cent[index] = vocab[word]\n","            else:\n","                cent[index] = vocab[word]\n","          data.append(cent)\n","  length=len(labels)\n","  data = torch.tensor(data, dtype=torch.long)\n","  labels = torch.tensor(labels, dtype=torch.long)\n","  print(len(data), len(labels))\n","  dataset = TensorDataset(data, labels)\n","  return DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, shuffle=True, drop_last=True), math.floor(length / batch_size), port, len(vocab)\n","\n","\n","def accuracy(data_iter, model, batch_count):\n","    prediction_labels, true_labels = [], []\n","    with torch.no_grad():\n","        for count, batch_datas in tqdm(enumerate(data_iter), desc='eval', total=batch_count):\n","            features, targets = batch_datas\n","            features, targets = features.cuda(), targets.cuda()\n","            output = model(features)\n","            pred = output.softmax(dim=1).argmax(dim=1).cpu()\n","            prediction_labels.append(pred.numpy())\n","            true_labels.append(targets.cpu().numpy())\n","    true_labels = np.concatenate(true_labels)\n","    prediction_labels = np.concatenate(prediction_labels)\n","    return classification_report(true_labels, prediction_labels,digits=3)\n","\n","if __name__==\"__main__\":\n","  attr=9\n","  train_m_dataloader,train_m_batch,p,vocab_len=mention_2_tensor(attr)\n","  model_m=Mention(vocab_len).cuda()\n","  test_m_dataloader,test_m_batch,_,_=mention_2_tensor(attr, context_path='./valid.txt', file_path='./data/valid/valid.csv')\n","  optimizer=torch.optim.Adam(model_m.parameters(),lr=5e-4)\n","  loss_func=nn.CrossEntropyLoss(torch.tensor(p,dtype=torch.float)).cuda()\n","  for epoch in range(100):\n","    start = time.time()\n","    model_m.train()\n","    train_loss_sum,train_acc_sum,n=0,0,0\n","    for step,m_data in tqdm(enumerate(train_m_dataloader),desc='train epoch:{}/{}'.format(epoch + 1, 100), total=train_m_batch):\n","      optimizer.zero_grad()\n","      m_train,m_label=m_data\n","      m_train,m_label=m_train.cuda(),m_label.cuda()\n","      l=model_m(m_train)\n","      loss=loss_func(l,m_label)\n","      loss.backward()\n","      optimizer.step()\n","      train_loss_sum+=loss.item()\n","      logits = l.softmax(dim=1)\n","      train_acc_sum += (logits.argmax(dim=1) == m_label).sum().item()\n","      n += m_label.shape[0]\n","    model_m.eval()\n","    result = accuracy(test_m_dataloader, model_m, test_m_batch)\n","    print('epoch %d, loss %.4f, train acc %.3f, time: %.3f' %\n","    (epoch + 1, train_loss_sum / n, train_acc_sum / n, (time.time() - start)))\n","    print(result)\n"]},{"cell_type":"markdown","metadata":{"id":"0h95qAdTnu5s"},"source":["5.模型三(对比实验)：基于Text-CNN的细粒度情感分析模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixkRiJXFnPy-"},"outputs":[],"source":["#情感分析，将-1,0,1进行分类\n","class SelfAttention(nn.Module):\n","    def __init__(self, input_size, hidden_size=300, num_of_header=3, hidden_droput=0.5):\n","        super(SelfAttention, self).__init__()\n","        if hidden_size % num_of_header != 0: raise ValueError()\n","        self.num_of_header = num_of_header\n","        self.attention_head_size = int(hidden_size/num_of_header)\n","        self.all_head_size = hidden_size\n","\n","        self.Q = nn.Linear(input_size, self.all_head_size)\n","        self.K = nn.Linear(input_size, self.all_head_size)\n","        self.V = nn.Linear(input_size, self.all_head_size)\n","        self.attn_dropout = nn.Dropout(hidden_droput)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_of_header, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, input_tensor):\n","        mixed_Q_layer = self.Q(input_tensor)\n","        mixed_K_layer = self.K(input_tensor)\n","        mixed_V_layer = self.V(input_tensor)\n","\n","        Q_layer = self.transpose_for_scores(mixed_Q_layer)\n","        K_layer = self.transpose_for_scores(mixed_K_layer)\n","        V_layer = self.transpose_for_scores(mixed_V_layer)\n","\n","        attention_scores = torch.matmul(Q_layer, K_layer.transpose(-1,-2))\n","        attention_scores /= math.sqrt(self.attention_head_size)\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","        attention_probs = self.attn_dropout(attention_probs)\n","\n","        Context_layer = torch.matmul(attention_probs, V_layer)\n","        Context_layer = Context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = Context_layer.size()[:-2] + (self.all_head_size,)\n","        Context_layer = Context_layer.view(*new_context_layer_shape)\n","        return Context_layer\n","\n","def text_2_tensor(label, max_len= 200, file_path='./trainer.txt'):\n","    vocab = dict()\n","    vocab['None'] = 0\n","    embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","    vocab_list = [k for k,_ in embedding_model.wv.key_to_index.items()]\n","    for i in range(len(vocab_list)):\n","        word = vocab_list[i]\n","        vocab[word] = i + 1\n","    data = []\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        pointer = 0\n","        while True:\n","            # pointer += 1\n","            line = f.readline()\n","            cent = [0 for i in range(max_len)]\n","            if len(line) == 0: break\n","            # if pointer % 3 != 0: continue\n","            sentence = line.split()\n","            for index,word in enumerate(sentence):\n","                if index >= max_len: break\n","                if word not in vocab.keys():\n","                    vocab[word] = len(vocab)\n","                    cent[index] = vocab[word]\n","                else:\n","                    cent[index] = vocab[word]\n","            data.append(cent)\n","    length=len(label)\n","    data = torch.tensor(data, dtype=torch.long)\n","    label = torch.tensor(label, dtype=torch.long)\n","    print(len(data), len(label))\n","    dataset = TensorDataset(data, label)\n","    return DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, drop_last=True), math.floor(length / batch_size), len(vocab)\n","\n","\n","class text_cnn(nn.Module):\n","    def __init__(self, embed_dim, class_num, kernel_num, kernel_sizes, dropout=0.5):\n","        super(TextCnn, self).__init__()\n","        # 词嵌入模型预训练\n","        embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","        word2idx = {'None': 0}\n","        vocab_list = [(k, embedding_model.wv[k]) for k, v in embedding_model.wv.key_to_index.items()]\n","        embeddings_matrix = np.zeros((len(embedding_model.wv.key_to_index.items()) + 1, embedding_model.vector_size))\n","        for i in range(len(vocab_list)):\n","            word = vocab_list[i][0]\n","            word2idx[word] = i + 1\n","            embeddings_matrix[i + 1] = vocab_list[i][1]\n","        embeddings_matrix = torch.tensor(embeddings_matrix, dtype=torch.float)\n","        # 初始化keras中的Embedding层权重\n","        self.embedding_table = nn.Embedding(len(embeddings_matrix), embed_dim, _weight= embeddings_matrix)\n","        self.embedding_table.requires_grad_ = False\n","        # 注意力机制\n","        # self.attn = SelfAttention(embed_dim, hidden_size=embed_dim, num_of_header=2, hidden_droput=0.4)\n","        # text_cnn\n","        Ci = 1\n","        Co = kernel_num\n","        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (f, embed_dim), padding=(2, 0)) for f in kernel_sizes])\n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(Co * len(kernel_sizes), class_num)\n","\n","    def forward(self, x):\n","        x = self.embedding_table(x.long())\n","        x = self.attn(x)\n","        x = x.unsqueeze(1)  # (N, Ci, token_num, embed_dim)\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, token_num) * len(kernel_sizes)]\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co) * len(kernel_sizes)]\n","        x = torch.cat(x, 1) # (N, Co * len(kernel_sizes))\n","        x = self.dropout(x)  # (N, Co * len(kernel_sizes))\n","        logit = self.fc(x)  # (N, class_num)\n","        return logit\n","\n","# 测试分类结果的准确性\n","def accuracy(data_iter, model, batch_count):\n","    prediction_labels, true_labels = [], []\n","    with torch.no_grad():\n","        for count, batch_datas in tqdm(enumerate(data_iter), desc='eval', total=batch_count):\n","            features, targets = batch_datas\n","            features, targets = features.cuda(), targets.cuda()\n","            output = model(features)\n","            pred = output.softmax(dim=1).argmax(dim=1).cpu()\n","            prediction_labels.append(pred.numpy())\n","            true_labels.append(targets.cpu().numpy())\n","    true_labels = np.concatenate(true_labels)\n","    prediction_labels = np.concatenate(prediction_labels)\n","    return classification_report(true_labels, prediction_labels)\n","\n","\n","\n","if __name__ == '__main__':\n","    # 所需参数\n","    attr = 17\n","    batch_size = 10\n","    embed_dim, class_num, kernel_num, kernel_sizes = 300, 4, 2, [2, 3, 4, 5]\n","    model = text_cnn(embed_dim, class_num, kernel_num, kernel_sizes).cuda()\n","    model_name = './' + get_attr_name(attr) + '.pt'\n","    # 获取训练数据\n","    train_labels, train_p = Get_Label(attr)\n","    train_data_loader, train_batch, vocab_len = text_2_tensor(train_labels)\n","    print(train_p)\n","    # 获取测试数据\n","    test_labels, _ = Get_Label(attr, file_path='./data/valid/valid.csv')\n","    test_data_loader, test_batch, _ = text_2_tensor(test_labels, file_path='./valid.txt')\n","    # 模型训练与测试\n","    loss_func = nn.CrossEntropyLoss(torch.tensor(train_p, dtype=torch.float)).cuda()\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-04)\n","    for epoch in range(100):\n","        start = time.time()\n","        model.train()\n","        train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n","        for step, batch_data in tqdm(enumerate(train_data_loader), desc='train epoch:{}/{}'.format(epoch + 1, 100), total=train_batch):\n","            feature, target = batch_data\n","            feature, target = feature.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","            logit = model(feature)\n","            loss = loss_func(logit,target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_sum += loss.item()\n","            logits = logit.softmax(dim=1)\n","            train_acc_sum += (logits.argmax(dim=1) == target).sum().item()\n","            n += target.shape[0]\n","        model.eval()\n","        result = accuracy(test_data_loader, model, test_batch)\n","        print('epoch %d, loss %.4f, train acc %.3f, time: %.3f' %\n","              (epoch + 1, train_loss_sum / n, train_acc_sum / n, (time.time() - start)))\n","        print(result)\n","        # if epoch==2: torch.save(model, model_name)\n"]},{"cell_type":"markdown","metadata":{"id":"9wrJtD7zoFrQ"},"source":["6.模型四：基于Text-CNN+LSTM+多头自注意力机制的细粒度情感分析模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gup03UH_MBhV"},"outputs":[],"source":["from torch.autograd import Variable\n","#情感分析，将-1,0,1进行分类\n","def text_2_tensor(label, max_len= 200, file_path='./trainer.txt'):\n","    vocab = dict()\n","    vocab['None'] = 0\n","    embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","    vocab_list = [k for k,_ in embedding_model.wv.key_to_index.items()]\n","    for i in range(len(vocab_list)):\n","        word = vocab_list[i]\n","        vocab[word] = i + 1\n","    data = []\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        pointer = 0\n","        while True:\n","            # pointer += 1\n","            line = f.readline()\n","            cent = [0 for i in range(max_len)]\n","            if len(line) == 0: break\n","            # if pointer % 3 != 0: continue\n","            sentence = line.split()\n","            for index,word in enumerate(sentence):\n","                if index >= max_len: break\n","                if word not in vocab.keys():\n","                    vocab[word] = len(vocab)\n","                    cent[index] = vocab[word]\n","                else:\n","                    cent[index] = vocab[word]\n","            data.append(cent)\n","    length=len(label)\n","    data = torch.tensor(data, dtype=torch.long)\n","    label = torch.tensor(label, dtype=torch.long)\n","    print(len(data), len(label))\n","    dataset = TensorDataset(data, label)\n","    return DataLoader(dataset=dataset, batch_size=batch_size, num_workers=4, drop_last=True), math.floor(length / batch_size), len(vocab)\n","\n","\n","class TCLA(nn.Module):\n","    def __init__(self, embed_dim, class_num, kernel_num, kernel_sizes, dropout=0.5):\n","        super(TextCnn, self).__init__()\n","        # 词嵌入模型预训练\n","        embedding_model = word2vec.Word2Vec.load('./trainer.model')\n","        word2idx = {'None': 0}\n","        vocab_list = [(k, embedding_model.wv[k]) for k, v in embedding_model.wv.key_to_index.items()]\n","        embeddings_matrix = np.zeros((len(embedding_model.wv.key_to_index.items()) + 1, embedding_model.vector_size))\n","        for i in range(len(vocab_list)):\n","            word = vocab_list[i][0]\n","            word2idx[word] = i + 1\n","            embeddings_matrix[i + 1] = vocab_list[i][1]\n","        embeddings_matrix = torch.tensor(embeddings_matrix, dtype=torch.float)\n","        # 初始化keras中的Embedding层权重\n","        self.embedding_table = nn.Embedding(len(embeddings_matrix), embed_dim, _weight= embeddings_matrix)\n","        self.embedding_table.requires_grad_ = False\n","        # text_cnn\n","        Ci = 1\n","        Co = kernel_num\n","        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (f, embed_dim), padding=(2, 0)) for f in kernel_sizes])\n","        self.dropout = nn.Dropout(dropout)\n","        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=256, num_layers=1, dropout=0.5, batch_first=True)\n","        self.lstm_attn = nn.Linear(256, class_num)\n","        self.word_attn = nn.Linear(200, 1)\n","        self.fc = nn.Linear(Co * len(kernel_sizes)+class_num, class_num)\n","\n","    def forward(self, y):\n","        h0 = Variable(torch.zeros(1, batch_size, 256)).cuda()\n","        c0 = Variable(torch.zeros(1, batch_size, 256)).cuda()\n","        y = self.embedding_table(y.long())\n","        x = y.unsqueeze(1)  # (N, Ci, token_num, embed_dim)\n","        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1]  # [(N, Co, token_num) * len(kernel_sizes)]\n","        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]  # [(N, Co) * len(kernel_sizes)]\n","        x = torch.cat(x, 1) # (N, Co * len(kernel_sizes))\n","        x = self.dropout(x)  # (N, Co * len(kernel_sizes))\n","        z,_ = self.lstm(y,(h0,c0))\n","        z = self.lstm_attn(F.relu(z))\n","        z = z.permute(0,2,1)\n","        z = self.word_attn(z).squeeze(2)\n","        logit = torch.cat((x,z),dim=1)\n","        logit = self.fc(logit)\n","        return logit\n","\n","if __name__ == '__main__':\n","    # 所需参数\n","    attr = 17\n","    batch_size = 10\n","    embed_dim, class_num, kernel_num, kernel_sizes = 300, 4, 2, [2, 3, 4, 5]\n","    model = TCLA(embed_dim, class_num, kernel_num, kernel_sizes).cuda()\n","    model_name = './' + get_attr_name(attr) + '.pt'\n","    # 获取训练数据\n","    train_labels, train_p = Get_Label(attr)\n","    train_data_loader, train_batch, vocab_len = text_2_tensor(train_labels)\n","    print(train_p)\n","    # 获取测试数据\n","    test_labels, _ = Get_Label(attr, file_path='./data/valid/valid.csv')\n","    test_data_loader, test_batch, _ = text_2_tensor(test_labels, file_path='./valid.txt')\n","    # 模型训练与测试\n","    loss_func = nn.CrossEntropyLoss(torch.tensor(train_p, dtype=torch.float)).cuda()\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-04)\n","    for epoch in range(100):\n","        start = time.time()\n","        model.train()\n","        train_loss_sum, train_acc_sum, n = 0.0, 0.0, 0\n","        for step, batch_data in tqdm(enumerate(train_data_loader), desc='train epoch:{}/{}'.format(epoch + 1, 100), total=train_batch):\n","            feature, target = batch_data\n","            feature, target = feature.cuda(), target.cuda()\n","            optimizer.zero_grad()\n","            logit = model(feature)\n","            loss = loss_func(logit,target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss_sum += loss.item()\n","            logits = logit.softmax(dim=1)\n","            train_acc_sum += (logits.argmax(dim=1) == target).sum().item()\n","            n += target.shape[0]\n","        model.eval()\n","        result = accuracy(test_data_loader, model, test_batch)\n","        print('epoch %d, loss %.4f, train acc %.3f, time: %.3f' %\n","              (epoch + 1, train_loss_sum / n, train_acc_sum / n, (time.time() - start)))\n","        print(result)\n","        # if epoch==2: torch.save(model, model_name)\n"]},{"cell_type":"markdown","metadata":{"id":"5P0HBV7-rp-5"},"source":["7.菜品名称口味聚类模型"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oAWIAMjI4ds"},"outputs":[],"source":["import joblib\n","import jieba\n","import re\n","import os\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import classification_report\n","\n","from sklearn.decomposition import LatentDirichletAllocation\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.cluster import KMeans\n","\n","# stopword = StopwordLoader()\n","\n","def sep_single(name):\n","    name_depart = jieba.cut(name)\n","    out = ''\n","    for word in name_depart:\n","        # \\u4e00-\\u9fa5为汉字的正则表达式,^表示对方括号中的正则表达式取补\n","        if word and not re.match(r'[^A-Z^a-z^0-9^\\u4e00-\\u9fa5]', word):\n","            out += word + ' '\n","    return out\n","\n","def sep_menu(file_path='./data/menu.csv',labels=['酸','甜','辣','咸','鲜']):\n","    # 将菜单分割为分词序列\n","    df = pd.read_csv(file_path, encoding='UTF-8', usecols=[0, 1])\n","    dish_name, l = [], []\n","    for index,row in df.iterrows():\n","        name_depart = sep_single(row['name'])\n","        # 将标签整型化\n","        for i in range(len(labels)):\n","          if row['taste']==labels[i]:\n","            l.append(i)\n","            break\n","        dish_name.append(name_depart)\n","    return dish_name, l\n","\n","def count_frequency(corpus, in_file):\n","    # 获取词频向量，os.path.exists()判断文件是否存在\n","    if os.path.exists(in_file):\n","        # 加载保存的模型\n","        cnt_vector = joblib.load(in_file)\n","        cnt_tf = cnt_vector.transform(corpus)\n","    else:\n","        cnt_vector = CountVectorizer()\n","        cnt_tf = cnt_vector.fit_transform(corpus)\n","        print('主题词袋:', len(cnt_vector.get_feature_names()))\n","        # 保存模型\n","        joblib.dump(cnt_vector, in_file)\n","    return cnt_tf\n","\n","def LDA(in_model, model_in_data, n=2):\n","    # 训练LDA模型\n","    if os.path.exists(in_model):\n","        # 加载已经训练好的模型\n","        lda = joblib.load(in_model)\n","        res = lda.transform(model_in_data)\n","    else:\n","        # 起初训练一个模型\n","        lda = LatentDirichletAllocation(n_components=n,\n","                        # max_iter=5,\n","                        # learning_method='online',\n","                        learning_offset=50.,\n","                        random_state=0)\n","        res = lda.fit_transform(model_in_data)\n","        joblib.dump(lda, in_model)\n","    return res\n","\n","def K_means(in_model, model_in_data, n=2):\n","    # 训练LDA模型\n","    if os.path.exists(in_model):\n","        # 加载已经训练好的模型\n","        kmeans = joblib.load(in_model)\n","        res = kmeans.transform(model_in_data)\n","    else:\n","        # 起初训练一个模型\n","        kmeans = KMeans(n_clusters=n)\n","        res = kmeans.fit_transform(model_in_data)\n","        joblib.dump(kmeans, in_model)\n","    return res\n","\n","def accuracy(pred, true_labels, file_path='./data/menu.csv'):\n","    pred_labels = []\n","    for prediction in pred:\n","        p = np.argmax(prediction)\n","        pred_labels.append(p)\n","    return classification_report(np.concatenate(true_labels), np.concatenate(pred_labels), digit=3)\n","\n","if __name__ == '__main__':\n","    # 设定口味类别标签 \n","    n = len(['酸','甜','辣','咸','鲜'])\n","    # 获取菜品名称数据和菜品标签\n","    data_list, dish_label = sep_menu()\n","\n","    # 训练LDA模型前需要先训练一个CountVectorizer模型\n","    cv_file = \"./CountVectorizer.pkl\"\n","    cnt_data_list = count_frequency(data_list, cv_file)\n","    lda_model = \"./lda_model.pk\"\n","    # 训练LDA模型并计算类别概率\n","    docres = LDA(lda_model, cnt_data_list, n=n)\n","    LDA_corpus = np.array(docres)\n","    # 计算LDA模型的准确率\n","    # 由于LDA只能确定哪些样本时同一类，不能指示类别标签，所以必须循环修改标签进行比对\n","    for i in range(n):\n","        print(accuracy(dish_label, docres))\n","        for j in range(len(dish_label)):\n","            dish_label[j] = (dish_label[j] + 1) % n\n","    # 举例测试LDA预测性能\n","    test_length = 6\n","    pre_list = ['杨梅', '香甜奶茶', '咖啡', '辣子鸡', '咖喱鸡', '蒸鱼']\n","    for i in range(test_length):\n","        pre_list[i] = sep_single(pre_list[i])\n","    pre_cnt_data_list = count_frequency(pre_list, cv_file)\n","    pre_docres = LDA(lda_model, pre_cnt_data_list, n=n)\n","    print('预测数据概率:\\n', np.array(pre_docres))\n","\n","    # 调用KMeans算法并获取分类结果\n","    km_model = \"./km_model.pk\"\n","    result = K_means(km_model, docres, n=n)\n","    KM_corpus = np.array(result)\n","    # 计算KMeans模型的准确率\n","    # 由于K-means只能确定哪些样本时同一类，不能指示类别标签，所以必须循环修改标签进行比对\n","    for i in range(n):\n","        print(accuracy(dish_label, result))\n","        for j in range(len(dish_label)):\n","            dish_label[j] = (dish_label[j] + 1) % n\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPbyvy4qQATYrQRJkfcWCHu","machine_shape":"hm","mount_file_id":"11ZiPkxXevEXxo_PQcMsOlsO2RyGrXh4G","name":"text_cnn.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
